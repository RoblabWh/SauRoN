{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b82888",
   "metadata": {},
   "source": [
    "## Aufgabe 1 \n",
    "## Implementierung einer Reward-Funktion für einen Reinforcement Learning (RL) Agenten\n",
    "\n",
    "In dieser Aufgabe sollen Sie eine Reward-Funktion für einen RL-Agenten implementieren. Der Agent hat die Aufgabe, ein bestimmtes Ziel zu erreichen und dabei möglichen statischen und dynamischen Hindernissen auszuweichen.\n",
    "\n",
    "Die Reward-Funktion ist ein zentrales Element in der Umgebung eines RL-Algorithmus. Sie bewertet die Aktionen des Agenten und liefert Rückmeldungen, die dem Agenten helfen, zu lernen und Entscheidungen zu treffen, die ihn seinem Ziel näherbringen. Ihre Aufgabe besteht darin, diese Funktion zu implementieren und dabei die Vielseitigkeit und Anpassungsfähigkeit einer solchen Funktion zu berücksichtigen.\n",
    "\n",
    "Folgen Sie diesen Schritten:\n",
    "\n",
    "1. **Erstellen Sie eine Reward-Funktion als Dictionary.** Jeder Eintrag in diesem Dictionary repräsentiert einen bestimmten Aspekt oder eine bestimmte Dimension der Bewertung. Beispielsweise kann ein Eintrag die Belohnung für das Erreichen des Ziels, eine Strafe für das Kollidieren mit einem Hindernis oder das Stehenbleiben des Agenten repräsentieren.\n",
    "\n",
    "2. **Überlegen Sie sich, welche Aspekte des Verhaltens des Agenten Sie belohnen oder bestrafen möchten,** und implementieren Sie diese in Ihrer Reward-Funktion. Einige Beispiele könnten sein: das Erreichen des Ziels, das Vermeiden von Kollisionen, das Halten einer bestimmten Distanz zum Ziel oder das Bewahren einer bestimmten Orientierung zum Ziel.\n",
    "\n",
    "3. **Nutzen Sie die Hilfsfunktionen der Robot-Klasse,** um wichtige Informationen über den Zustand des Roboters zu erhalten. Diese Funktionen können Informationen über die Position und Ausrichtung des Roboters, die Entfernung zu Hindernissen und ähnliches liefern. Überlegen Sie, wie Sie diese Informationen nutzen können, um die Belohnungen und Strafen in Ihrer Reward-Funktion zu berechnen. \n",
    "\n",
    "> `is_staying_in_place(robot.last_positions)` Diese Funktion prüft, ob der Roboter in den letzten Positionen stillgestanden hat. Das ist wichtig zu wissen, da es Anzeichen dafür sein könnte, dass der Roboter in einem Hindernis feststeckt oder in einer Schleife feststeckt, in der er immer wieder die gleiche Aktion ausführt.\n",
    "\n",
    "> `robot.initialGoalDist` Diese Variable speichert die ursprüngliche Entfernung des Roboters zum Ziel, als der Lernprozess begonnen hat. Dieser Wert kann dazu genutzt werden, um zu überprüfen, ob der Roboter näher am Ziel ist als zu Beginn, was ein gutes Zeichen für Fortschritt wäre.\n",
    "\n",
    "> `currentLinVel = np.around(robot.state_raw[robot.time_steps - 1][4], decimals=5)` Dieser Ausdruck berechnet die aktuelle lineare Geschwindigkeit des Roboters. robot.state_raw[robot.time_steps - 1][4] greift auf den neuesten Status des Roboters zu und holt die lineare Geschwindigkeit, die dann auf 5 Dezimalstellen gerundet wird.\n",
    "\n",
    "> `lastLinVel = np.around(robot.state_raw[robot.time_steps - 2][4], decimals=5)` Ähnlich wie der vorherige Ausdruck berechnet dieser die lineare Geschwindigkeit des Roboters im vorherigen Zeitschritt. Der einzige Unterschied ist, dass hier robot.time_steps - 2 verwendet wird, um auf den vorherigen Status des Roboters zuzugreifen.\n",
    "\n",
    "> `currentAngVel = np.around(robot.state_raw[robot.time_steps - 1][5], decimals=5)` Dieser Ausdruck berechnet die aktuelle Winkelgeschwindigkeit des Roboters, ähnlich wie die Berechnung der linearen Geschwindigkeit.\n",
    "\n",
    "> `lastAngVel = np.around(robot.state_raw[robot.time_steps - 2][5], decimals=5)` Wie der vorherige Ausdruck berechnet dieser die Winkelgeschwindigkeit des Roboters im vorherigen Zeitschritt.\n",
    "\n",
    "Das bereitgestellte Grundgerüst soll Ihnen dabei helfen, diese Aufgabe zu bewältigen. Denken Sie daran, dass Ihre Reward-Funktion flexibel und anpassungsfähig sein sollte, um auf die spezifischen Anforderungen des RL-Problems eingehen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1d0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import is_staying_in_place\n",
    "\n",
    "def createReward(robot, dist_new, dist_old, reachedPickup, collision, runOutOfTime):\n",
    "    \"\"\"\n",
    "    Creates a (sparse) reward based on the euklidian distance, if the robot has reached his goal and if the robot\n",
    "    collided with a wall or another robot.\n",
    "\n",
    "    :param robot: robot\n",
    "    :param dist_new: the new distance to goal (after the action has been taken)\n",
    "    :param dist_old: the old distance to goal (before the action has been taken)\n",
    "    :param reachedPickup: True if the robot reached his goal in this step\n",
    "    :param collision: True if the robot collided with a wall or another robot\n",
    "    :param runOutOfTime: True if the maximum steps were taken by the robot and its still alive\n",
    "    :return: returns the result of the fitness function\n",
    "    \"\"\"\n",
    "\n",
    "    reward = {}\n",
    "    # reward['nothing'] = 0\n",
    "    # reward['anothernothing'] = 0\n",
    "\n",
    "    reward = {}\n",
    "    r_arrival = 10 # reward for reaching the goal\n",
    "    r_collision = -10 # Robot crashed with a wall or another robot\n",
    "    r_run_out_of_time = -5 # Robot has run out of time\n",
    "    r_stop = -0.01 # Robot stood still\n",
    "    w_g = 0.1\n",
    "    w_d = 15\n",
    "    w_gn = 0\n",
    "    w_w = -0.1\n",
    "    w_p = 0.1\n",
    "    a_p = 0.045 # weight for the angle, always positive\n",
    "\n",
    "    if reachedPickup:\n",
    "        reward['arrival'] = r_arrival\n",
    "    elif runOutOfTime:\n",
    "        reward['out_of_time'] = r_run_out_of_time\n",
    "    elif collision:\n",
    "        reward['collision'] = r_collision #* living_factor\n",
    "    else:\n",
    "        # Distance Reward\n",
    "        if dist_old > dist_new:\n",
    "            reward['dist'] = w_g #w_g * (dist_old - dist_new)\n",
    "        else:\n",
    "            reward['dist'] = w_gn #w_gn * (dist_old - dist_new)\n",
    "\n",
    "        if np.min(robot.get_robot_states()[0][0]) < 0.03:\n",
    "            reward['wall'] = w_w\n",
    "    \n",
    "    # Ihr könnt beliebig viele Dictionary Einträge für die unterschiedlichen Aspekte der\n",
    "    # Reward Funktion erstellen. In tensorboard(weiter unten wird es beschrieben) werden diese\n",
    "    # dann unter dem Namen angezeigt\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb95ee4",
   "metadata": {},
   "source": [
    "## Return, Advantage und GAE\n",
    "\n",
    "In der Welt des Reinforcement Learning (RL) verwenden wir oft Begriffe wie \"Vorteil\" (Advantage) und \"Rückgabe\" (Return) zur Beschreibung bestimmter Konzepte.\n",
    "\n",
    "1. **Return:** Im Kontext von RL bezieht sich Return auf die gesamte zukünftige Belohnung, die ein Agent von einem bestimmten Zustand aus erwartet. Es wird oft als Diskontierte Summe zukünftiger Belohnungen bezeichnet. Hierbei wird eine Diskontierungsrate (oft Gamma genannt) verwendet, die die Wichtigkeit zukünftiger Belohnungen im Vergleich zur aktuellen Belohnung verringert. Formell kann der Return als `Gt = Rt+1 + γRt+2 + γ^2Rt+3 + ...` berechnet werden, wobei `Rt+1, Rt+2, ...` zukünftige Belohnungen sind und γ die Diskontierungsrate ist.\n",
    "\n",
    "2. **Advantage:** Advantage ist eine Methode zur Quantifizierung, wie viel besser eine bestimmte Aktion in einem bestimmten Zustand ist, im Vergleich zu dem, was wir im Durchschnitt von diesem Zustand erwarten würden. Es misst im Grunde genommen den Unterschied zwischen der Q-Funktion und der V-Funktion in RL. Die Q-Funktion gibt den erwarteten Return für ein gegebenes Zustands-Aktions-Paar an, während die V-Funktion den erwarteten Return für einen gegebenen Zustand unabhängig von der spezifischen Aktion angibt. Die Advantage-Funktion ist dann definiert als `A(s, a) = Q(s, a) - V(s)`.\n",
    "\n",
    "Die Berechnung von Advantage und Return ist zentral für viele RL-Algorithmen, da sie hilft zu bestimmen, welche Aktionen besser als andere sind und welche Zustände wertvoller sind. Sie werden oft verwendet, um die Richtung zu bestimmen, in die sich die Parameter unseres Modells bewegen sollten, um das Lernen zu verbessern.\n",
    "\n",
    "Einige Algorithmen, wie der Proximal Policy Optimization (PPO) Algorithmus, verwenden eine Technik namens Generalized Advantage Estimation (GAE) zur Berechnung der Advantage. GAE ist eine Methode zur Verwendung von gewichteten Summen mehrerer Returns, um die Schätzung zu verbessern. Es hat zwei Parameter, die Diskontierungsrate γ und einen weiteren Parameter λ, der bestimmt, wie die Gewichtung zwischen verschiedenen Returns vorgenommen wird. In Ihrer Funktion get_advantages wird GAE verwendet, um den Advantage zu berechnen, der dann zur Verbesserung der Policy und Value-Funktionen des Modells verwendet wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daceb6f",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "\n",
    "## Implementierung der GAE Funktion für Proximal Policy Optimization\n",
    "In dieser Aufgabe sollen Sie die Berechnung von Advantages und Returns für den Proximal Policy Optimization (PPO) Algorithmus implementieren. Dabei wird die Technik der Generalized Advantage Estimation (GAE) verwendet, um die Advantages zu berechnen.\n",
    "\n",
    "#### masks\n",
    "Die Variable masks wird verwendet, um das Ende einer Episode zu kennzeichnen. In der Welt des Reinforcement Learning ist eine Episode eine Reihe von Interaktionen zwischen dem Agenten und der Umgebung, die mit dem Erreichen eines Endzustands endet.\n",
    "\n",
    "Wenn Sie eine Umgebung haben, in der Episoden enden können (wie zum Beispiel in dieser Simulation, die endet, wenn der Agent ein bestimmtes Ziel erreicht, ein bestimmter Zeitraum abgelaufen ist oder der Agent gecrasht ist), dann verwenden Sie oft eine Maske, um zu kennzeichnen, wo diese Endpunkte sind.\n",
    "\n",
    "Diese Masken werden später im Code verwendet, um sicherzustellen, dass der zukünftige Return korrekt berechnet wird, wenn eine Episode endet. Wenn eine Episode endet, gibt es keine zukünftige Belohnung mehr zu berücksichtigen, so dass die Diskontierte Summe zukünftiger Belohnungen ab diesem Punkt aufhören sollte. Die `masks`-Variable hilft dabei, diese Punkte zu kennzeichnen, so dass die zukünftigen Belohnungen korrekt berechnet werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb156ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages(gamma, _lambda, values, masks, rewards):\n",
    "    \"\"\"\n",
    "    Computes the advantages of the given rewards and values.\n",
    "\n",
    "    :param values: The values of the states.\n",
    "    :param masks: The masks of the states.\n",
    "    :param rewards: The rewards of the states.\n",
    "    :return: The advantages of the states.\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    returns = []\n",
    "    gae = 0\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        # TODO: Berechnen Sie den Temporal Difference Fehler 'delta'\n",
    "        delta = None\n",
    "\n",
    "        # TODO: Berechnen Sie die Generalized Advantage Estimate 'gae'\n",
    "        gae = None\n",
    "\n",
    "        # TODO: Berechnen Sie den Return und fügen Sie ihn zur Liste der Returns hinzu\n",
    "        return_ = None\n",
    "        returns.insert(0, return_)\n",
    "\n",
    "    advantages = torch.FloatTensor(advantages).to(device)\n",
    "    norm_adv = (advantages - advantages.mean()) / (advantages.std() + 1e-10) if advantages.numel() > 1 else advantages\n",
    "\n",
    "    returns = torch.FloatTensor(returns).to(device)\n",
    "\n",
    "    return norm_adv, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afef2952-4b71-407c-b598-9d744b63fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages(gamma, _lambda, values, masks, rewards):\n",
    "    \"\"\"\n",
    "    Computes the advantages of the given rewards and values.\n",
    "\n",
    "    :param values: The values of the states.\n",
    "    :param masks: The masks of the states.\n",
    "    :param rewards: The rewards of the states.\n",
    "    :return: The advantages of the states.\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    returns = []\n",
    "    gae = 0\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "        gae = delta + gamma * _lambda * masks[i] * gae\n",
    "        advantages.insert(0, gae)\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    advantages = torch.FloatTensor(advantages).to(device)\n",
    "    norm_adv = (advantages - advantages.mean()) / (advantages.std() + 1e-10) if advantages.numel() > 1 else advantages\n",
    "\n",
    "    returns = torch.FloatTensor(returns).to(device)\n",
    "\n",
    "    assert not torch.isnan(norm_adv).any(), f\"Advantages are NaN: {norm_adv}\"\n",
    "\n",
    "    return norm_adv, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bf526",
   "metadata": {},
   "source": [
    "### Simulationsumgebung\n",
    "\n",
    "Es gibt verschiedene Parameter, die Sie anpassen können, um das Training zu steuern. Die einzelnen Parameter sind in den Kommentaren erklärt. \n",
    "\n",
    "Nachdem Sie alle Parameter nach Ihren Bedürfnissen angepasst haben, können Sie die Funktion startSimulation aufrufen und das Training beginnen. Die ausgewählten Level-Dateien werden als Trainingsumgebungen verwendet, und die Funktionen createReward und get_advantages werden verwendet, um die Belohnungen und Vorteile zu berechnen, die für das Training des Agenten benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0481ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from ./models/mymodel/PPO_continuous_model_best.pth\n",
      "Episode 1 \t Length: 31 \t Reward: -10.7\n",
      "Episode 2 \t Length: 926 \t Reward: 243.5999999999976\n",
      "Episode 3 \t Length: 176 \t Reward: 26.89999999999997\n",
      "Episode 4 \t Length: 184 \t Reward: 27.199999999999974\n"
     ]
    }
   ],
   "source": [
    "import PyQt5.QtGui\n",
    "from startSim import startSimulation\n",
    "import random\n",
    "\n",
    "args = {}\n",
    "level_files = ['ez.svg', 'ez2.svg','ez4.svg', 'Simple.svg', 'tunnel2.svg']\n",
    "#level_files = ['kreuzung.svg', 'Funnel.svg', 'tunnel2.svg', 'ez3.svg'] + ['ez.svg', 'ez2.svg','ez4.svg', 'Simple.svg']# Model Parameters\n",
    "random.shuffle(level_files)\n",
    "\n",
    "# Dies ist der Pfad zum Ordner, in dem die trainierten Modelle gespeichert werden.\n",
    "args['ckpt_folder']= './models/mymodel'\n",
    "# Der Name des trainierten Modells, das gespeichert oder geladen werden soll.\n",
    "# Beim testen schaut genau nach welches model ihr wählen wollt \"model_best\" \"model_solved\" oder \"model_final\"\n",
    "args['model_name']= 'model_best'\n",
    "# Der Modus, in dem die Simulation laufen soll. Hier ist es auf \"train\" gesetzt, was \n",
    "# bedeutet, dass wir ein Training durchführen. 'train' oder 'test'\n",
    "args['mode']= 'test'\n",
    "# 'small' oder 'big' network\n",
    "args['inputspace'] = 'big'\n",
    "\n",
    "# Train Parameters\n",
    "\n",
    "# Wenn auf True gesetzt, wird das Modell von einer früheren Checkpoint-Datei wiederhergestellt \n",
    "# und das Training wird fortgesetzt. Wenn auf False gesetzt, wird das Modell von Grund auf neu trainiert.\n",
    "args['restore']=False\n",
    "# Die Anzahl der Schritte, die in jeder Episode durchgeführt werden sollen.\n",
    "args['steps']=1000\n",
    "# Die Anzahl der Schritte, die durchgeführt werden sollen, bevor trainiert wird.\n",
    "args['update_experience']=2500\n",
    "# Die Anzahl der Minibatches zum Aufteilen der Trainingsdaten während einer Trainingsepoche.\n",
    "args['batches']=32\n",
    "# Die Anzahl der Epochen, für die das Modell trainiert werden soll.\n",
    "args['K_epochs']=10\n",
    "# Der Diskontierungsfaktor, der verwendet wird, um zukünftige Belohnungen zu bewerten.\n",
    "args['gamma']=0.99\n",
    "# Der Faktor Lambda im Generalized Advantage Estimation (GAE) Algorithmus, der dazu dient, den \n",
    "# Bias-Variance-Tradeoff bei der Schätzung der Vorteile zu steuern.\n",
    "args['_lambda']=0.99\n",
    "# Die Lernrate, die den Schrittgrößenparameter beim Aktualisieren der Gewichte des Modells bestimmt.\n",
    "args['lr']=0.0002\n",
    "\n",
    "startSimulation(args, level_files, createReward, get_advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f4e17",
   "metadata": {},
   "source": [
    "## Visualisierung mit tensorboard\n",
    "\n",
    "TensorBoard ist ein Visualisierungstool für Machine Learning. Es ermöglicht es den Benutzern, die verschiedenen Aspekte ihres Machine Learning-Modells zu visualisieren und zu verstehen. Dabei können die Lernkurven von Modellen, Histogramme von Gewichtungen oder Biases, Bilder oder Texte, die durch das Modell generiert werden, sowie viele andere Dinge visualisiert werden.\n",
    "\n",
    "Im Kontext von Reinforcement Learning kann TensorBoard dazu verwendet werden, Metriken wie den durchschnittlichen Reward, die durchschnittliche Anzahl von Schritten pro Episode oder andere benutzerdefinierte Metriken zu verfolgen. Diese Metriken können im Laufe der Zeit verfolgt und visualisiert werden, um zu verstehen, wie das Modell lernt und sich verbessert (oder auch nicht).\n",
    "\n",
    "Sie können es starten, indem Sie `tensorboard --logdir=pfad_zu_ihren_logs` in Ihrer Konsole ausführen. Hierbei ist pfad_zu_ihren_logs der Pfad zu dem Verzeichnis, in dem die Log-Dateien gespeichert sind, die von SauRoN während des Trainings erstellt wurden. SauRoN erstellt diese Dateien unter dem von Ihnen angegeben `ckpt_folder`.\n",
    "\n",
    "![Reward](./reward_tb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1aba5",
   "metadata": {},
   "source": [
    "#### Hinweise\n",
    "\n",
    "Reinforcement-Learning beginnt oft mit kleinen, aber ermutigenden Fortschritten. Schon nach wenigen Trainingszyklen sollten Sie in der Lage sein, erste positive Ergebnisse sowohl in der Simulation als auch in TensorBoard zu beobachten. Ein konkreter Erfolg in dieser Lernumgebung könnte beispielsweise sein, dass Ihre Agenten lernen, Kollisionen zu vermeiden oder bestimmte Wegpunkte zu erreichen.\n",
    "\n",
    "Dennoch ist es wichtig, realistische Erwartungen zu haben: Das Erreichen einer 100%igen Erfolgsrate beim Erreichen des Ziels ist eine beachtliche Herausforderung und erfordert wahrscheinlich viel mehr Training, als man zunächst annimmt. Machen Sie sich jedoch keine Sorgen, wenn der Fortschritt nicht sofort sichtbar ist – das ist völlig normal und Teil des Lernprozesses.\n",
    "\n",
    "Sie haben die Freiheit, das Training jederzeit zu unterbrechen und später fortzusetzen. Der aktuell beste Stand Ihres Modells wird kontinuierlich gespeichert, sodass Sie keine Fortschritte verlieren. Dies ermöglicht eine flexible Anpassung des Lernprozesses an Ihren Zeitplan.\n",
    "\n",
    "Darüber hinaus bietet die Simulation die Möglichkeit, die aktuellen Gewichtungen zu speichern. Dies kann besonders wertvoll sein, wenn Sie bestimmte Gewichtungen für zukünftige Experimente, Analysen oder Anwendungen beibehalten möchten.\n",
    "\n",
    "#### Tipps zum Testen und Experimentieren\n",
    "\n",
    "**Schrittweise Anpassung der Parameter:** Beginnen Sie mit kleineren Werten für die Lernrate, das Diskontierungsfaktor Gamma und die Anzahl der Trainingsepochen. Beobachten Sie, wie sich die Leistung des Agenten ändert, wenn Sie diese Werte schrittweise erhöhen.\n",
    "\n",
    "**Nutzen Sie TensorBoard:** TensorBoard ist ein großartiges Werkzeug, um den Lernprozess visuell zu überwachen. Verwenden Sie es, um Belohnungen, Verluste und andere Metriken im Laufe der Zeit zu verfolgen. Dies kann Ihnen helfen, zu verstehen, wie gut Ihr Agent lernt und wann Sie möglicherweise Anpassungen vornehmen müssen.\n",
    "\n",
    "**Experimentieren Sie mit verschiedenen Umgebungen:** Die bereitgestellten Level-Dateien repräsentieren verschiedene Umgebungen, die unterschiedliche Herausforderungen für den Agenten darstellen. Versuchen Sie, Ihr Modell in verschiedenen Umgebungen zu trainieren und zu testen, um zu sehen, wie gut es sich generalisieren kann.\n",
    "\n",
    "**Auswertung der Ergebnisse:** Vergessen Sie nicht, Ihre trainierten Modelle auch zu testen. Sie können dies tun, indem Sie den Modus auf 'test' setzen und die Simulation starten. Dies gibt Ihnen eine gute Vorstellung davon, wie gut Ihr Agent in einer unabhängigen Umgebung abschneidet.\n",
    "\n",
    "**Training auf unterschiedlichen Umgebungen** Das Training auf unterschiedlichen Umgebungen (auch bekannt als Curriculum Learning) kann sehr nützlich sein, insbesondere in komplexen Umgebungen, in denen ein RL-Agent Schwierigkeiten haben könnte, relevante Strategien zu lernen. Durch den Start in einfacheren Umgebungen kann der Agent grundlegende Strategien lernen, die ihm in schwierigeren Umgebungen helfen.\n",
    "\n",
    "**Frühes Stoppen des Trainings**\n",
    "Frühes Stoppen ist eine Technik, die in vielen Arten von Machine Learning verwendet wird, um Overfitting zu vermeiden. Im Kontext von Reinforcement Learning kann es auch dazu verwendet werden, um unnötiges Training zu vermeiden, wenn der Agent bereits eine akzeptable Leistung erreicht hat.\n",
    "\n",
    "Die Idee ist, das Training zu stoppen, sobald die Leistung des Agenten aufhört, sich signifikant zu verbessern, oder sogar zu beginnen, sich zu verschlechtern. In Reinforcement Learning könnte dies bedeuten, dass das Training gestoppt wird, wenn die durchschnittliche Belohnung über eine Reihe von Episoden hinweg konstant bleibt oder zu sinken beginnt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
