{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b82888",
   "metadata": {},
   "source": [
    "## Aufgabe 1 \n",
    "## Implementierung einer Reward-Funktion für einen Reinforcement Learning (RL) Agenten\n",
    "\n",
    "In dieser Aufgabe sollen Sie eine Reward-Funktion für einen RL-Agenten implementieren. Der Agent hat die Aufgabe, ein bestimmtes Ziel zu erreichen und dabei möglichen statischen und dynamischen Hindernissen auszuweichen.\n",
    "\n",
    "Die Reward-Funktion ist ein zentrales Element in der Umgebung eines RL-Algorithmus. Sie bewertet die Aktionen des Agenten und liefert Rückmeldungen, die dem Agenten helfen, zu lernen und Entscheidungen zu treffen, die ihn seinem Ziel näherbringen. Ihre Aufgabe besteht darin, diese Funktion zu implementieren und dabei die Vielseitigkeit und Anpassungsfähigkeit einer solchen Funktion zu berücksichtigen.\n",
    "\n",
    "Folgen Sie diesen Schritten:\n",
    "\n",
    "1. **Erstellen Sie eine Reward-Funktion als Dictionary.** Jeder Eintrag in diesem Dictionary repräsentiert einen bestimmten Aspekt oder eine bestimmte Dimension der Bewertung. Beispielsweise kann ein Eintrag die Belohnung für das Erreichen des Ziels, eine Strafe für das Kollidieren mit einem Hindernis oder das Stehenbleiben des Agenten repräsentieren.\n",
    "\n",
    "2. **Überlegen Sie sich, welche Aspekte des Verhaltens des Agenten Sie belohnen oder bestrafen möchten,** und implementieren Sie diese in Ihrer Reward-Funktion. Einige Beispiele könnten sein: das Erreichen des Ziels, das Vermeiden von Kollisionen, das Halten einer bestimmten Distanz zum Ziel oder das Bewahren einer bestimmten Orientierung zum Ziel.\n",
    "\n",
    "3. **Nutzen Sie die Hilfsfunktionen der Robot-Klasse,** um wichtige Informationen über den Zustand des Roboters zu erhalten. Diese Funktionen können Informationen über die Position und Ausrichtung des Roboters, die Entfernung zu Hindernissen und ähnliches liefern. Überlegen Sie, wie Sie diese Informationen nutzen können, um die Belohnungen und Strafen in Ihrer Reward-Funktion zu berechnen. \n",
    "\n",
    "> `is_staying_in_place(robot.last_positions)` Diese Funktion prüft, ob der Roboter in den letzten Positionen stillgestanden hat. Das ist wichtig zu wissen, da es Anzeichen dafür sein könnte, dass der Roboter in einem Hindernis feststeckt oder in einer Schleife feststeckt, in der er immer wieder die gleiche Aktion ausführt.\n",
    "\n",
    "> `robot.initialGoalDist` Diese Variable speichert die ursprüngliche Entfernung des Roboters zum Ziel, als der Lernprozess begonnen hat. Dieser Wert kann dazu genutzt werden, um zu überprüfen, ob der Roboter näher am Ziel ist als zu Beginn, was ein gutes Zeichen für Fortschritt wäre.\n",
    "\n",
    "> `currentLinVel = np.around(robot.state_raw[robot.time_steps - 1][4], decimals=5)` Dieser Ausdruck berechnet die aktuelle lineare Geschwindigkeit des Roboters. robot.state_raw[robot.time_steps - 1][4] greift auf den neuesten Status des Roboters zu und holt die lineare Geschwindigkeit, die dann auf 5 Dezimalstellen gerundet wird.\n",
    "\n",
    "> `lastLinVel = np.around(robot.state_raw[robot.time_steps - 2][4], decimals=5)` Ähnlich wie der vorherige Ausdruck berechnet dieser die lineare Geschwindigkeit des Roboters im vorherigen Zeitschritt. Der einzige Unterschied ist, dass hier robot.time_steps - 2 verwendet wird, um auf den vorherigen Status des Roboters zuzugreifen.\n",
    "\n",
    "> `currentAngVel = np.around(robot.state_raw[robot.time_steps - 1][5], decimals=5)` Dieser Ausdruck berechnet die aktuelle Winkelgeschwindigkeit des Roboters, ähnlich wie die Berechnung der linearen Geschwindigkeit.\n",
    "\n",
    "> `lastAngVel = np.around(robot.state_raw[robot.time_steps - 2][5], decimals=5)` Wie der vorherige Ausdruck berechnet dieser die Winkelgeschwindigkeit des Roboters im vorherigen Zeitschritt.\n",
    "\n",
    "Das bereitgestellte Grundgerüst soll Ihnen dabei helfen, diese Aufgabe zu bewältigen. Denken Sie daran, dass Ihre Reward-Funktion flexibel und anpassungsfähig sein sollte, um auf die spezifischen Anforderungen des RL-Problems eingehen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1d0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import is_staying_in_place\n",
    "\n",
    "def createReward(robot, dist_new, dist_old, reachedPickup, collision, runOutOfTime):\n",
    "    \"\"\"\n",
    "    Creates a (sparse) reward based on the euklidian distance, if the robot has reached his goal and if the robot\n",
    "    collided with a wall or another robot.\n",
    "\n",
    "    :param robot: robot\n",
    "    :param dist_new: the new distance to goal (after the action has been taken)\n",
    "    :param dist_old: the old distance to goal (before the action has been taken)\n",
    "    :param reachedPickup: True if the robot reached his goal in this step\n",
    "    :param collision: True if the robot collided with a wall or another robot\n",
    "    :param runOutOfTime: True if the maximum steps were taken by the robot and its still alive\n",
    "    :return: returns the result of the fitness function\n",
    "    \"\"\"\n",
    "\n",
    "    reward = {}\n",
    "    reward['nothing'] = 0\n",
    "    reward['anothernothing'] = 0\n",
    "    \n",
    "    # Ihr könnt beliebig viele Dictionary Einträge für die unterschiedlichen Aspekte der\n",
    "    # Reward Funktion erstellen. In tensorboard(weiter unten wird es beschrieben) werden diese\n",
    "    # dann unter dem Namen angezeigt\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb95ee4",
   "metadata": {},
   "source": [
    "## Return, Advantage und GAE\n",
    "\n",
    "In der Welt des Reinforcement Learning (RL) verwenden wir oft Begriffe wie \"Vorteil\" (Advantage) und \"Rückgabe\" (Return) zur Beschreibung bestimmter Konzepte.\n",
    "\n",
    "1. **Return:** Im Kontext von RL bezieht sich Return auf die gesamte zukünftige Belohnung, die ein Agent von einem bestimmten Zustand aus erwartet. Es wird oft als Diskontierte Summe zukünftiger Belohnungen bezeichnet. Hierbei wird eine Diskontierungsrate (oft Gamma genannt) verwendet, die die Wichtigkeit zukünftiger Belohnungen im Vergleich zur aktuellen Belohnung verringert. Formell kann der Return als `Gt = Rt+1 + γRt+2 + γ^2Rt+3 + ...` berechnet werden, wobei `Rt+1, Rt+2, ...` zukünftige Belohnungen sind und γ die Diskontierungsrate ist.\n",
    "\n",
    "2. **Advantage:** Advantage ist eine Methode zur Quantifizierung, wie viel besser eine bestimmte Aktion in einem bestimmten Zustand ist, im Vergleich zu dem, was wir im Durchschnitt von diesem Zustand erwarten würden. Es misst im Grunde genommen den Unterschied zwischen der Q-Funktion und der V-Funktion in RL. Die Q-Funktion gibt den erwarteten Return für ein gegebenes Zustands-Aktions-Paar an, während die V-Funktion den erwarteten Return für einen gegebenen Zustand unabhängig von der spezifischen Aktion angibt. Die Advantage-Funktion ist dann definiert als `A(s, a) = Q(s, a) - V(s)`.\n",
    "\n",
    "Die Berechnung von Advantage und Return ist zentral für viele RL-Algorithmen, da sie hilft zu bestimmen, welche Aktionen besser als andere sind und welche Zustände wertvoller sind. Sie werden oft verwendet, um die Richtung zu bestimmen, in die sich die Parameter unseres Modells bewegen sollten, um das Lernen zu verbessern.\n",
    "\n",
    "Einige Algorithmen, wie der Proximal Policy Optimization (PPO) Algorithmus, verwenden eine Technik namens Generalized Advantage Estimation (GAE) zur Berechnung der Advantage. GAE ist eine Methode zur Verwendung von gewichteten Summen mehrerer Returns, um die Schätzung zu verbessern. Es hat zwei Parameter, die Diskontierungsrate γ und einen weiteren Parameter λ, der bestimmt, wie die Gewichtung zwischen verschiedenen Returns vorgenommen wird. In Ihrer Funktion get_advantages wird GAE verwendet, um den Advantage zu berechnen, der dann zur Verbesserung der Policy und Value-Funktionen des Modells verwendet wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daceb6f",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "\n",
    "## Implementierung der GAE Funktion für Proximal Policy Optimization\n",
    "In dieser Aufgabe sollen Sie die Berechnung von Advantages und Returns für den Proximal Policy Optimization (PPO) Algorithmus implementieren. Dabei wird die Technik der Generalized Advantage Estimation (GAE) verwendet, um die Advantages zu berechnen.\n",
    "\n",
    "#### masks\n",
    "Die Variable masks wird verwendet, um das Ende einer Episode zu kennzeichnen. In der Welt des Reinforcement Learning ist eine Episode eine Reihe von Interaktionen zwischen dem Agenten und der Umgebung, die mit dem Erreichen eines Endzustands endet.\n",
    "\n",
    "Wenn Sie eine Umgebung haben, in der Episoden enden können (wie zum Beispiel in dieser Simulation, die endet, wenn der Agent ein bestimmtes Ziel erreicht, ein bestimmter Zeitraum abgelaufen ist oder der Agent gecrasht ist), dann verwenden Sie oft eine Maske, um zu kennzeichnen, wo diese Endpunkte sind.\n",
    "\n",
    "Diese Masken werden später im Code verwendet, um sicherzustellen, dass der zukünftige Return korrekt berechnet wird, wenn eine Episode endet. Wenn eine Episode endet, gibt es keine zukünftige Belohnung mehr zu berücksichtigen, so dass die Diskontierte Summe zukünftiger Belohnungen ab diesem Punkt aufhören sollte. Die `masks`-Variable hilft dabei, diese Punkte zu kennzeichnen, so dass die zukünftigen Belohnungen korrekt berechnet werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb156ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages(gamma, _lambda, values, masks, rewards):\n",
    "    \"\"\"\n",
    "    Computes the advantages of the given rewards and values.\n",
    "\n",
    "    :param values: The values of the states.\n",
    "    :param masks: The masks of the states.\n",
    "    :param rewards: The rewards of the states.\n",
    "    :return: The advantages of the states.\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    returns = []\n",
    "    gae = 0\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        # TODO: Berechnen Sie den Temporal Difference Fehler 'delta'\n",
    "        delta = None\n",
    "\n",
    "        # TODO: Berechnen Sie die Generalized Advantage Estimate 'gae'\n",
    "        gae = None\n",
    "\n",
    "        # TODO: Berechnen Sie den Return und fügen Sie ihn zur Liste der Returns hinzu\n",
    "        return_ = None\n",
    "        returns.insert(0, return_)\n",
    "\n",
    "    advantages = torch.FloatTensor(advantages).to(device)\n",
    "    norm_adv = (advantages - advantages.mean()) / (advantages.std() + 1e-10) if advantages.numel() > 1 else advantages\n",
    "\n",
    "    returns = torch.FloatTensor(returns).to(device)\n",
    "\n",
    "    return norm_adv, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28721bd5-e5ce-4197-b9d7-0f729d5e79c4",
   "metadata": {},
   "source": [
    "## Aufgabe 3\n",
    "## Erstellen eines Eingabemoduls für ein Reinforcement Learning (RL) neuronales Netz\n",
    "\n",
    "In dieser Aufgabe sollen Sie ein Eingabemodul für ein neuronales Netz in PyTorch implementieren, das mehrere unterschiedliche Eingaben verarbeitet und zu einer gemeinsamen Repräsentation zusammenführt. Dieses Eingabemodul ist Teil einer größeren Architektur, die später im Rahmen von Reinforcement Learning (z. B. PPO) trainiert wird.\n",
    "\n",
    "Das Netz erhält verschiedene Eingangsdaten über den Zustand eines Roboters, darunter:\n",
    "- Lidar-Scans über mehrere Zeitschritte (z. B. 4 Zeitschritte)\n",
    "- Orientierung des Roboters zum Ziel\n",
    "- Entfernung zum Ziel\n",
    "- Geschwindigkeit des Roboters (lineare und rotatorische Komponenten)\n",
    "\n",
    "Ziel ist es, diese Datenquellen getrennt mit passenden Schichten (z. B. Convolutional Layer für die Lidar-Daten und Dense Layer für die anderen Inputs) zu verarbeiten, ihre Features zu extrahieren und am Ende zu einem flachen, konsolidierten Vektor zu kombinieren. Dieser Vektor kann anschließend von weiteren Netzwerkschichten (z. B. Policy- und Value-Netzwerk) genutzt werden. Diese Netze müssen Sie nicht erstellen.\n",
    "\n",
    "## Vorgehen\n",
    "1. Erstellen Sie eine eigene Klasse, die von `nn.Module` erbt (z. B. BigInput).\n",
    "2. Verarbeiten Sie jede Eingabekomponente getrennt, z. B. mithilfe von Convolutional- oder Linear-Schichten. Diese Einzelschritte sind nicht vorgeschrieben, sollten aber dem Typ der Eingabedaten angemessen sein.\n",
    "3. Kombinieren Sie die resultierenden Feature-Vektoren zu einem gemeinsamen Eingabevektor.\n",
    "4. Implementieren Sie die Methode forward(self, ...), die die Eingaben entgegennimmt, verarbeitet und die finale Feature-Repräsentation zurückgibt. Diese Methode definiert den Ablauf der Vorwärtsauswertung des Netzwerks.\n",
    "5. Der Rückgabewert der forward-Methode muss ein Tensor der Form (Batchgröße, `self.out_features`) sein.\n",
    "6. Der Wert von self.out_features gibt die Anzahl der Ausgabeneuronen des Moduls an und muss zwingend gesetzt werden. Andere Module im Netzwerk werden sich auf diese festgelegte Ausgabegröße verlassen.\n",
    "7. Nutzen Sie die Hilfsfunktion `initialize_hidden_weights`, um die Gewichte der Layer initial zu setzen (diese Funktion wird vorab bereitgestellt).\n",
    "\n",
    "Sie dürfen frei entscheiden, wie viele Schichten, Aktivierungsfunktionen oder Verarbeitungsschritte Sie zwischen Eingabe und Ausgabe verwenden – wichtig ist, dass die Ausgabeform korrekt ist und alle Eingabedaten sinnvoll verarbeitet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10e1f75-a8a5-4c31-abe8-1738f59d4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import initialize_hidden_weights\n",
    "\n",
    "class BigInput(nn.Module):\n",
    "    def __init__(self, scan_size):\n",
    "        \"\"\"\n",
    "        Eingabemodul für verschiedene Roboterzustände im Reinforcement Learning.\n",
    "\n",
    "        :param scan_size: Anzahl der Lidar-Werte pro Zeitschritt.\n",
    "        \"\"\"\n",
    "        super(BigInput, self).__init__()\n",
    "\n",
    "        self.time_steps = 4  # Anzahl vergangener Zeitschritte\n",
    "        self.out_features = 32  # Muss gesetzt werden – Ausgabegröße des Moduls\n",
    "\n",
    "        # Beispielhafte Layer (Platzhalter – durch eigene Verarbeitung ersetzen)\n",
    "        self.lidar_layer = nn.Identity()\n",
    "        self.orientation_layer = nn.Identity()\n",
    "        self.distance_layer = nn.Identity()\n",
    "        self.velocity_layer = nn.Identity()\n",
    "        self.final_layer = nn.Identity()\n",
    "\n",
    "    def forward(self, lidar, orientation, distance, velocity):\n",
    "        \"\"\"\n",
    "        Verarbeitet alle Eingaben und gibt einen zusammengeführten Feature-Vektor aus.\n",
    "\n",
    "        :param lidar: Tensor mit Lidar-Daten (z. B. [B, T, S])\n",
    "        :param orientation: Tensor mit Zielorientierung (z. B. [B, T, 2])\n",
    "        :param distance: Tensor mit Distanz zum Ziel (z. B. [B, T, 1])\n",
    "        :param velocity: Tensor mit Geschwindigkeit (z. B. [B, T, 2])\n",
    "        :return: Tensor der Form (Batchgröße, self.out_features)\n",
    "        \"\"\"\n",
    "        lidar_feat = self.lidar_layer(lidar)\n",
    "        orientation_feat = self.orientation_layer(orientation)\n",
    "        distance_feat = self.distance_layer(distance)\n",
    "        velocity_feat = self.velocity_layer(velocity)\n",
    "\n",
    "        combined = torch.cat((lidar_feat, orientation_feat, distance_feat, velocity_feat), dim=1)\n",
    "        output = self.final_layer(combined)\n",
    "        return output\n",
    "\n",
    "    def get_in_features(self, h_in, layers_dict):\n",
    "        \"\"\"\n",
    "        Berechnet die Ausgabelänge eines 1D-Convolutional Layers nach mehreren Verarbeitungsschritten.\n",
    "\n",
    "        Diese Funktion ist nützlich, wenn Sie mehrere Conv1d-Schichten definieren und die resultierende\n",
    "        Feature-Länge vor der Verwendung in einem Linear-Layer ermitteln möchten.\n",
    "\n",
    "        :param h_in: Ausgangslänge des Eingabevektors (z. B. die Lidar-Scan-Länge)\n",
    "        :param layers_dict: Liste von Dictionaries mit Layer-Parametern:\n",
    "               [{'kernel_size': ..., 'stride': ..., 'padding': ..., 'dilation': ...}, ...]\n",
    "        :return: berechnete Länge nach allen Convolution-Schritten\n",
    "        \"\"\"\n",
    "        for layer in layers_dict:\n",
    "            kernel_size = layer['kernel_size']\n",
    "            stride = layer['stride']\n",
    "            padding = layer['padding']\n",
    "            dilation = layer['dilation']\n",
    "            h_in = ((h_in + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1\n",
    "        return int(h_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bf526",
   "metadata": {},
   "source": [
    "### Simulationsumgebung\n",
    "\n",
    "Es gibt verschiedene Parameter, die Sie anpassen können, um das Training zu steuern. Die einzelnen Parameter sind in den Kommentaren erklärt. \n",
    "\n",
    "Nachdem Sie alle Parameter nach Ihren Bedürfnissen angepasst haben, können Sie die Funktion startSimulation aufrufen und das Training beginnen. Die ausgewählten Level-Dateien werden als Trainingsumgebungen verwendet, und die Funktionen createReward und get_advantages werden verwendet, um die Belohnungen und Vorteile zu berechnen, die für das Training des Agenten benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0481ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from ./models/mymodel/PPO_continuous_model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nex/Dokumente/Code/SauRoN/PPO/Algorithm.py:222: UserWarning: Could not restore model from models/mymodel/PPO_continuous_model_best.pth. Falling back to train mode.\n",
      "  warnings.warn(f\"Could not restore model from {path}. Falling back to train mode.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1081 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Die Lernrate, die den Schrittgrößenparameter beim Aktualisieren der Gewichte des Modells bestimmt.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0002\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mstartSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateReward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_advantages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dokumente/Code/SauRoN/startSim.py:64\u001b[0m, in \u001b[0;36mstartSimulation\u001b[0;34m(args, level_files, createReward, get_advantages)\u001b[0m\n\u001b[1;32m     57\u001b[0m     train(args\u001b[38;5;241m.\u001b[39mmodel_name, env, inputspace\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39minputspace, solved_percentage\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msolved_percentage,\n\u001b[1;32m     58\u001b[0m           max_episodes\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_episodes, max_timesteps\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msteps, update_experience\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mupdate_experience,\n\u001b[1;32m     59\u001b[0m           _lambda\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39m_lambda, K_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mK_epochs, eps_clip\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39meps_clip,\n\u001b[1;32m     60\u001b[0m           gamma\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgamma, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.990\u001b[39m], ckpt_folder\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mckpt_folder,\n\u001b[1;32m     61\u001b[0m           restore\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mrestore, log_interval\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlog_interval, scan_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnumber_of_rays,\n\u001b[1;32m     62\u001b[0m           batches\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatches, tensorboard\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtensorboard)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.990\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscan_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantages_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_advantages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dokumente/Code/SauRoN/PPO/Environment.py:133\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(env_name, env, render, inputspace, _lambda, K_epochs, eps_clip, gamma, lr, betas, ckpt_folder, test_episodes, scan_size, advantages_func)\u001b[0m\n\u001b[1;32m    130\u001b[0m observations \u001b[38;5;241m=\u001b[39m statesToObservationsTensor(states)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Run old policy\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action_certain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m states, rewards, dones, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(torchToNumpy(actions))\n\u001b[1;32m    137\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28msum\u001b[39m([value \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m reward\u001b[38;5;241m.\u001b[39mvalues()]) \u001b[38;5;28;01mfor\u001b[39;00m reward \u001b[38;5;129;01min\u001b[39;00m rewards])\n",
      "File \u001b[0;32m~/Dokumente/Code/SauRoN/PPO/Algorithm.py:229\u001b[0m, in \u001b[0;36mPPO.select_action_certain\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action_certain\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations):\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_certain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dokumente/Code/SauRoN/PPO/Algorithm.py:135\u001b[0m, in \u001b[0;36mActorCritic.act_certain\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    134\u001b[0m     laser, orientation, distance, velocity \u001b[38;5;241m=\u001b[39m states\n\u001b[0;32m--> 135\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlaser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvelocity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/anaconda3/envs/sauron/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Dokumente/Code/SauRoN/PPO/Algorithm.py:43\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, laser, orientation_to_goal, distance_to_goal, velocity)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, laser, orientation_to_goal, distance_to_goal, velocity):\n\u001b[0;32m---> 43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInputspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlaser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientation_to_goal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_to_goal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvelocity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     mu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu(x))\n\u001b[1;32m     45\u001b[0m     std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std)\n",
      "File \u001b[0;32m~/anaconda3/envs/sauron/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m, in \u001b[0;36mBigInput.forward\u001b[0;34m(self, lidar, orientation, distance, velocity)\u001b[0m\n\u001b[1;32m     37\u001b[0m distance_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_layer(distance)\n\u001b[1;32m     38\u001b[0m velocity_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelocity_layer(velocity)\n\u001b[0;32m---> 40\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientation_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvelocity_feat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(combined)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1081 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import PyQt5.QtGui\n",
    "from startSim import startSimulation\n",
    "import random\n",
    "\n",
    "args = {}\n",
    "level_files = ['ez.svg', 'ez2.svg','ez4.svg', 'Simple.svg', 'tunnel2.svg']\n",
    "#level_files = ['kreuzung.svg', 'Funnel.svg', 'tunnel2.svg', 'ez3.svg'] + ['ez.svg', 'ez2.svg','ez4.svg', 'Simple.svg']# Model Parameters\n",
    "random.shuffle(level_files)\n",
    "\n",
    "# Dies ist der Pfad zum Ordner, in dem die trainierten Modelle gespeichert werden.\n",
    "args['ckpt_folder']= './models/mymodel'\n",
    "# Der Name des trainierten Modells, das gespeichert oder geladen werden soll.\n",
    "# Beim testen schaut genau nach welches model ihr wählen wollt \"model_best\" \"model_solved\" oder \"model_final\"\n",
    "args['model_name']= 'model_best'\n",
    "# Der Modus, in dem die Simulation laufen soll. Hier ist es auf \"train\" gesetzt, was \n",
    "# bedeutet, dass wir ein Training durchführen. 'train' oder 'test'\n",
    "args['mode']= 'test'\n",
    "# 'small', 'big' or custom network \n",
    "args['inputspace'] = BigInput\n",
    "\n",
    "# Train Parameters\n",
    "\n",
    "# Wenn auf True gesetzt, wird das Modell von einer früheren Checkpoint-Datei wiederhergestellt \n",
    "# und das Training wird fortgesetzt. Wenn auf False gesetzt, wird das Modell von Grund auf neu trainiert.\n",
    "args['restore']=False\n",
    "# Die Anzahl der Schritte, die in jeder Episode durchgeführt werden sollen.\n",
    "args['steps']=1000\n",
    "# Die Anzahl der Schritte, die durchgeführt werden sollen, bevor trainiert wird.\n",
    "args['update_experience']=2500\n",
    "# Die Anzahl der Minibatches zum Aufteilen der Trainingsdaten während einer Trainingsepoche.\n",
    "args['batches']=32\n",
    "# Die Anzahl der Epochen, für die das Modell trainiert werden soll.\n",
    "args['K_epochs']=10\n",
    "# Der Diskontierungsfaktor, der verwendet wird, um zukünftige Belohnungen zu bewerten.\n",
    "args['gamma']=0.99\n",
    "# Der Faktor Lambda im Generalized Advantage Estimation (GAE) Algorithmus, der dazu dient, den \n",
    "# Bias-Variance-Tradeoff bei der Schätzung der Vorteile zu steuern.\n",
    "args['_lambda']=0.99\n",
    "# Die Lernrate, die den Schrittgrößenparameter beim Aktualisieren der Gewichte des Modells bestimmt.\n",
    "args['lr']=0.0002\n",
    "\n",
    "startSimulation(args, level_files, createReward, get_advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f4e17",
   "metadata": {},
   "source": [
    "## Visualisierung mit tensorboard\n",
    "\n",
    "TensorBoard ist ein Visualisierungstool für Machine Learning. Es ermöglicht es den Benutzern, die verschiedenen Aspekte ihres Machine Learning-Modells zu visualisieren und zu verstehen. Dabei können die Lernkurven von Modellen, Histogramme von Gewichtungen oder Biases, Bilder oder Texte, die durch das Modell generiert werden, sowie viele andere Dinge visualisiert werden.\n",
    "\n",
    "Im Kontext von Reinforcement Learning kann TensorBoard dazu verwendet werden, Metriken wie den durchschnittlichen Reward, die durchschnittliche Anzahl von Schritten pro Episode oder andere benutzerdefinierte Metriken zu verfolgen. Diese Metriken können im Laufe der Zeit verfolgt und visualisiert werden, um zu verstehen, wie das Modell lernt und sich verbessert (oder auch nicht).\n",
    "\n",
    "Sie können es starten, indem Sie `tensorboard --logdir=pfad_zu_ihren_logs` in Ihrer Konsole ausführen. Hierbei ist pfad_zu_ihren_logs der Pfad zu dem Verzeichnis, in dem die Log-Dateien gespeichert sind, die von SauRoN während des Trainings erstellt wurden. SauRoN erstellt diese Dateien unter dem von Ihnen angegeben `ckpt_folder`.\n",
    "\n",
    "![Reward](./reward_tb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1aba5",
   "metadata": {},
   "source": [
    "#### Hinweise\n",
    "\n",
    "Reinforcement-Learning beginnt oft mit kleinen, aber ermutigenden Fortschritten. Schon nach wenigen Trainingszyklen sollten Sie in der Lage sein, erste positive Ergebnisse sowohl in der Simulation als auch in TensorBoard zu beobachten. Ein konkreter Erfolg in dieser Lernumgebung könnte beispielsweise sein, dass Ihre Agenten lernen, Kollisionen zu vermeiden oder bestimmte Wegpunkte zu erreichen.\n",
    "\n",
    "Dennoch ist es wichtig, realistische Erwartungen zu haben: Das Erreichen einer 100%igen Erfolgsrate beim Erreichen des Ziels ist eine beachtliche Herausforderung und erfordert wahrscheinlich viel mehr Training, als man zunächst annimmt. Machen Sie sich jedoch keine Sorgen, wenn der Fortschritt nicht sofort sichtbar ist – das ist völlig normal und Teil des Lernprozesses.\n",
    "\n",
    "Sie haben die Freiheit, das Training jederzeit zu unterbrechen und später fortzusetzen. Der aktuell beste Stand Ihres Modells wird kontinuierlich gespeichert, sodass Sie keine Fortschritte verlieren. Dies ermöglicht eine flexible Anpassung des Lernprozesses an Ihren Zeitplan.\n",
    "\n",
    "Darüber hinaus bietet die Simulation die Möglichkeit, die aktuellen Gewichtungen zu speichern. Dies kann besonders wertvoll sein, wenn Sie bestimmte Gewichtungen für zukünftige Experimente, Analysen oder Anwendungen beibehalten möchten.\n",
    "\n",
    "#### Tipps zum Testen und Experimentieren\n",
    "\n",
    "**Schrittweise Anpassung der Parameter:** Beginnen Sie mit kleineren Werten für die Lernrate, das Diskontierungsfaktor Gamma und die Anzahl der Trainingsepochen. Beobachten Sie, wie sich die Leistung des Agenten ändert, wenn Sie diese Werte schrittweise erhöhen.\n",
    "\n",
    "**Nutzen Sie TensorBoard:** TensorBoard ist ein großartiges Werkzeug, um den Lernprozess visuell zu überwachen. Verwenden Sie es, um Belohnungen, Verluste und andere Metriken im Laufe der Zeit zu verfolgen. Dies kann Ihnen helfen, zu verstehen, wie gut Ihr Agent lernt und wann Sie möglicherweise Anpassungen vornehmen müssen.\n",
    "\n",
    "**Experimentieren Sie mit verschiedenen Umgebungen:** Die bereitgestellten Level-Dateien repräsentieren verschiedene Umgebungen, die unterschiedliche Herausforderungen für den Agenten darstellen. Versuchen Sie, Ihr Modell in verschiedenen Umgebungen zu trainieren und zu testen, um zu sehen, wie gut es sich generalisieren kann.\n",
    "\n",
    "**Auswertung der Ergebnisse:** Vergessen Sie nicht, Ihre trainierten Modelle auch zu testen. Sie können dies tun, indem Sie den Modus auf 'test' setzen und die Simulation starten. Dies gibt Ihnen eine gute Vorstellung davon, wie gut Ihr Agent in einer unabhängigen Umgebung abschneidet.\n",
    "\n",
    "**Training auf unterschiedlichen Umgebungen** Das Training auf unterschiedlichen Umgebungen (auch bekannt als Curriculum Learning) kann sehr nützlich sein, insbesondere in komplexen Umgebungen, in denen ein RL-Agent Schwierigkeiten haben könnte, relevante Strategien zu lernen. Durch den Start in einfacheren Umgebungen kann der Agent grundlegende Strategien lernen, die ihm in schwierigeren Umgebungen helfen.\n",
    "\n",
    "**Frühes Stoppen des Trainings**\n",
    "Frühes Stoppen ist eine Technik, die in vielen Arten von Machine Learning verwendet wird, um Overfitting zu vermeiden. Im Kontext von Reinforcement Learning kann es auch dazu verwendet werden, um unnötiges Training zu vermeiden, wenn der Agent bereits eine akzeptable Leistung erreicht hat.\n",
    "\n",
    "Die Idee ist, das Training zu stoppen, sobald die Leistung des Agenten aufhört, sich signifikant zu verbessern, oder sogar zu beginnen, sich zu verschlechtern. In Reinforcement Learning könnte dies bedeuten, dass das Training gestoppt wird, wenn die durchschnittliche Belohnung über eine Reihe von Episoden hinweg konstant bleibt oder zu sinken beginnt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
